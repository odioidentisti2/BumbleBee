# -*- coding: utf-8 -*-
"""ESA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mip52UJ7VZXoygeHZMDNUIUd2WHOWJf3
"""

# !pip install rdkit

import torch

# Controlla se Ã¨ disponibile una GPU
if torch.cuda.is_available():
    print("GPU disponibile!")
    # Stampa il nome della GPU
    print("Nome GPU:", torch.cuda.get_device_name(0))
    # Stampa la versione del toolkit CUDA
    print("Versione CUDA:", torch.version.cuda)
else:
    print("Nessuna GPU disponibile.")

import torch
import torch.nn as nn
import torch.nn.functional as F

#DATA
from rdkit import Chem
import csv
from torch.utils.data import Dataset, DataLoader


class MoleculeDataset(Dataset):
    TOX_MAP = {'Mutagenic': 1, 'NON-Mutagenic': 0}

    def __init__(self, filepath):
        self.data = []
        with open(filepath, 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                label_str = row['Experimental_value']
                if label_str not in MoleculeDataset.TOX_MAP:
                    continue  # Salta righe con etichette sconosciute
                label = MoleculeDataset.TOX_MAP[label_str]
                smiles = row['SMILES']
                d = smiles_to_data(smiles, float(label))
                if d is not None:
                    self.data.append(d)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]


# Feature extractors faithful to ESA (from repo logic)
def one_hot_encoding(x, allowable_set):
    return [int(x == s) for s in allowable_set]

def atom_features(atom):
    return torch.tensor(
        one_hot_encoding(atom.GetAtomicNum(), list(range(1, 119))) +
        one_hot_encoding(atom.GetDegree(), list(range(0, 11))) +
        one_hot_encoding(atom.GetFormalCharge(), [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]) +
        one_hot_encoding(int(atom.GetHybridization()), list(range(6))) +
        [int(atom.GetIsAromatic())],
        dtype=torch.float
    )

def bond_features(bond):
    return torch.tensor(
        one_hot_encoding(bond.GetBondType(), [
            Chem.rdchem.BondType.SINGLE,
            Chem.rdchem.BondType.DOUBLE,
            Chem.rdchem.BondType.TRIPLE,
            Chem.rdchem.BondType.AROMATIC
        ]) +
        [int(bond.GetIsConjugated()), int(bond.IsInRing())],
        dtype=torch.float
    )

def smiles_to_data(smiles, label):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None
    x = torch.stack([atom_features(atom) for atom in mol.GetAtoms()])
    edge_attr = []
    src, dst = [], []
    for bond in mol.GetBonds():
        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()
        feat = bond_features(bond)
        src += [i, j]
        dst += [j, i]
        edge_attr += [feat, feat]
    edge_attr = torch.stack(edge_attr)
    edge_index = torch.tensor([src, dst], dtype=torch.long)
    y = torch.tensor([label], dtype=torch.float)
    return {"x": x, "edge_index": edge_index, "edge_attr": edge_attr, "y": y}

class MAB(nn.Module):
    def __init__(self, dim_Q, dim_K, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.fc_q = nn.Linear(dim_Q, dim_K)
        self.fc_k = nn.Linear(dim_K, dim_K)
        self.fc_v = nn.Linear(dim_K, dim_K)
        self.fc_o = nn.Linear(dim_K, dim_K)
        self.ln0 = nn.LayerNorm(dim_K)
        self.ln1 = nn.LayerNorm(dim_K)
        self.fc_r = nn.Sequential(
            nn.Linear(dim_K, dim_K),
            nn.ReLU(),
            nn.Linear(dim_K, dim_K)
        )

    def forward(self, Q, K):
        Q_ = self.fc_q(Q)
        K_ = self.fc_k(K)
        V_ = self.fc_v(K)
        Q_ = torch.cat(Q_.split(Q_.size(-1) // self.num_heads, dim=2), dim=0)
        K_ = torch.cat(K_.split(K_.size(-1) // self.num_heads, dim=2), dim=0)
        V_ = torch.cat(V_.split(V_.size(-1) // self.num_heads, dim=2), dim=0)
        A = torch.softmax(Q_.bmm(K_.transpose(1, 2)) / (K_.size(-1) ** 0.5), dim=2)
        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), dim=0), dim=2)
        O = self.ln0(O)
        O = O + self.fc_r(O)
        return self.ln1(O)

class SAB(nn.Module):
    def __init__(self, dim_in, dim_out, num_heads):
        super().__init__()
        self.mab = MAB(dim_in, dim_out, num_heads)
    def forward(self, X):
        return self.mab(X, X)

class PMA(nn.Module):
    def __init__(self, dim, num_heads, num_seeds):
        super().__init__()
        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))
        nn.init.xavier_uniform_(self.S)
        self.mab = MAB(dim, dim, num_heads)
    def forward(self, X):
        S = self.S.repeat(X.size(0), 1, 1)
        return self.mab(S, X)

class ESAMoleculeClassifier(nn.Module):
    def __init__(self, node_dim, edge_dim, hidden_dim=128, output_dim=1, num_heads=4, num_inds=1):
        super().__init__()
        self.node_proj = nn.Sequential(nn.Linear(node_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))
        self.edge_proj = nn.Sequential(nn.Linear(edge_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))
        self.edge_set_proj = nn.Sequential(
            nn.Linear(hidden_dim * 2 + hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        self.encoder = nn.Sequential(
            SAB(hidden_dim, hidden_dim, num_heads),
            SAB(hidden_dim, hidden_dim, num_heads),
            SAB(hidden_dim, hidden_dim, num_heads)
        )
        self.pooling = PMA(hidden_dim, num_heads, num_inds)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * num_inds, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, batch_data):
        edge_feats_list = []
        for data in batch_data:
            x, edge_index, edge_attr = data['x'], data['edge_index'], data['edge_attr']
            h = self.node_proj(x)
            e = self.edge_proj(edge_attr)
            src, dst = edge_index
            edge_inputs = torch.cat([h[src], h[dst], e], dim=1)
            edge_feats = self.edge_set_proj(edge_inputs)
            edge_feats_list.append(edge_feats)

        max_edges = max(f.size(0) for f in edge_feats_list)
        d = edge_feats_list[0].size(1)
        B = len(edge_feats_list)
        E = torch.zeros(B, max_edges, d, device=edge_feats_list[0].device)
        for i, f in enumerate(edge_feats_list):
            E[i, :f.size(0)] = f

        for layer in self.encoder:
            E = layer(E)

        pooled = self.pooling(E)
        logits = self.classifier(pooled.view(pooled.size(0), -1)).squeeze(-1)
        return logits
def collate_fn(batch):
    return batch

def train(model, train_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    for batch in train_loader:
        for item in batch:
            item['x'] = item['x'].to(device)
            item['edge_index'] = item['edge_index'].to(device)
            item['edge_attr'] = item['edge_attr'].to(device)
            item['y'] = item['y'].to(device)
        optimizer.zero_grad()
        logits = model(batch)
        labels = torch.stack([item['y'] for item in batch]).squeeze(-1)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * len(batch)
        preds = (torch.sigmoid(logits) > 0.5).float()
        correct += (preds == labels).sum().item()
        total += len(batch)
    return total_loss / total, correct / total

def main(train_loader):
    if torch.cuda.is_available():
        device = torch.device('cuda')
        print('CUDA')
    else:
        device = torch.device('cpu')
        print('CPU')

    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    sample = next(iter(train_loader))[0]
    node_dim = sample['x'].size(-1)
    edge_dim = sample['edge_attr'].size(-1)
    model = ESAMoleculeClassifier(node_dim, edge_dim).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.BCEWithLogitsLoss()

    for epoch in range(1, 51):
        loss, acc = train(model, train_loader, optimizer, criterion, device)
        print(f"Epoch {epoch:02d} - Loss: {loss:.4f} - Accuracy: {acc:.4f}")

    print("Training complete.")
    return model

# Load dataset
dataset = MoleculeDataset('DATASETS/MUTA_SARPY_4204.csv')
train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)
main(train_loader)